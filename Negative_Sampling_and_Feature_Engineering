{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Negative_Sampling_and_Feature_Engineering","provenance":[{"file_id":"1hTniN8DrrC1VTmn7t4DP6Vp-FP3-Rhr8","timestamp":1605170377505},{"file_id":"1otagSjHSAky-rdc95T3E0qaYN9DSJbVp","timestamp":1604157997818},{"file_id":"1IA5UnXa1Rw6PBkepg6al-683HSVp4dzo","timestamp":1599853076594},{"file_id":"1N56HkengJ9FZlw-gBiRbo6q10Gsz71ip","timestamp":1599673999737},{"file_id":"155S_bb3viIoL0wAwkIyr1r8XQu4ARwA9","timestamp":1599673914124}],"collapsed_sections":["gQUFcX5MZNBR","_yrYl22OD5c2","-MWW4Mb2jclA","zNyYq9VqJBMn","MVfadOhzwIi9","djDt_rfSPqVQ","LJADtWOPw9RW","0xa7e7Eijp7q","TTYvY4twpYQ3","BYEjQDP3qxH5","mTqY2cI_dxbv"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"FG5OxtPHw3Zh"},"source":["from scipy import stats\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import tensorflow as tf\n","import random\n","\n","import keras\n","from keras.preprocessing.sequence import pad_sequences\n","\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","%matplotlib inline\n","import seaborn as sns\n","from pylab import rcParams\n","\n","from sklearn import preprocessing\n","from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split, GroupShuffleSplit\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PzdaXn8F2AD5"},"source":["#download train data\n","data = drive.CreateFile({'id': 'here provide the file id from link sharing in from google drive'})\n","data.GetContentFile('data_cleaned.pkl')\n","\n","file = open('data_cleaned.pkl','rb')\n","train = pickle.load(file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vfThJcabEFZy"},"source":["## Negative Sampling"]},{"cell_type":"markdown","metadata":{"id":"Wz3M_Tom9los"},"source":["#### Negative Sampling"]},{"cell_type":"code","metadata":{"id":"5QGT6BFS5QE_"},"source":["# get product attributes data\n","products = drive.CreateFile({'id': 'here provide the file id from link sharing in from google drive'})\n","products.GetContentFile('product_attributes_data.tsv.gz')\n","products = pd.read_table('product_attributes_data.tsv.gz')\n","products.rename(columns={'rpid' : 'product_id'}, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YJ9ktqcirVNv"},"source":["#merge with transactions\n","df_concat = pd.merge(train, products, how='left', on=['product_id'])\n","# create week, year and month\n","df_concat.loc[df_concat['day'] <= '2016-01-03', 'year'] = '2015' \n","df_concat.loc[df_concat['day'] > '2016-01-03', 'year'] = '2016'\n","df_concat.loc[df_concat['day'] >= '2017-01-02', 'year'] = '2017' \n","df_concat['day'] = pd.to_datetime(df_concat['day'])\n","df_concat['week'] = df_concat['day'].dt.week.astype('str')\n","df_concat['month'] = df_concat['day'].dt.month.astype('str')\n","df_concat['yearandmonth'] = df_concat['year'] + df_concat['month']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UtpspevXFEcs"},"source":["# get unit price by dividing by the quantity\n","df_concat['price'] = df_concat['price'] / df_concat['quantity']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d7H2nuHYqOYC"},"source":["# remove missing category and subcategory names\n","df_concat = df_concat[df_concat['category_name'].notna()]\n","df_concat = df_concat[df_concat['subcategory_name'].notna()]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EtMIugrbE5_I"},"source":["We first try to sample a negative from the subcategory."]},{"cell_type":"code","metadata":{"id":"mgUBhaeXirqe"},"source":["df_single_sub_categories = df_concat.groupby(['subcategory_name', 'yearandmonth'], as_index=False).agg({'product_id' : 'nunique'})\n","single_sub_categories = df_single_sub_categories[df_single_sub_categories['product_id'] == 1].subcategory_name.to_list()\n","df_subcategory_sampling = df_concat[~df_concat['subcategory_name'].isin(single_sub_categories)]\n","df_subcategory_sampling = df_subcategory_sampling.reset_index()\n","df_subcategory_sampling = df_subcategory_sampling[['article_text', 'subcategory_name', 'yearandmonth']]\n","df_subcategory_sampling = df_subcategory_sampling.drop_duplicates()\n","#merged this info with actual df using subcategory name\n","df_subcategory_sampling = df_subcategory_sampling[df_subcategory_sampling['subcategory_name'].notna()]\n","df_subcategory_sampling['subcategory_rival'] = [np.random.choice(df_subcategory_sampling[(df_subcategory_sampling['subcategory_name']==cat) & (df_subcategory_sampling['yearandmonth']==ynm) & (df_subcategory_sampling['article_text']!=iden)]['article_text']) for cat, ynm, iden in zip(df_subcategory_sampling['subcategory_name'], df_subcategory_sampling['yearandmonth'], df_subcategory_sampling['article_text'])]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qb0LRXRwE2tl"},"source":["If the subcategory consist of only a single products, we sample a product from the same category."]},{"cell_type":"code","metadata":{"id":"lsG8xrCkm2py"},"source":["df_single_categories = df_concat.groupby(['category_name', 'yearandmonth'], as_index=False).agg({'product_id' : 'nunique'})\n","single_categories = df_single_categories[df_single_categories['product_id'] == 1].category_name.to_list()\n","df_category_sampling = df_concat[~df_concat['category_name'].isin(single_categories)]\n","df_category_sampling = df_category_sampling.reset_index()\n","df_category_sampling = df_category_sampling[['article_text', 'category_name', 'yearandmonth']]\n","df_category_sampling = df_category_sampling.drop_duplicates()\n","#merged this info with actual df using category name\n","df_category_sampling = df_category_sampling[df_category_sampling['category_name'].notna()]\n","df_category_sampling['category_rival'] = [np.random.choice(df_category_sampling[(df_category_sampling['category_name']==cat) & (df_category_sampling['yearandmonth']==ynm) & (df_category_sampling['article_text']!=iden)]['article_text']) for cat, ynm, iden in zip(df_category_sampling['category_name'], df_category_sampling['yearandmonth'], df_category_sampling['article_text'])]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v1QaGQ2bITez"},"source":["all_products = df_concat[['article_text', 'yearandmonth']]\n","all_products = all_products.drop_duplicates()\n","# get them in a df\n","all_products = pd.merge(all_products, df_subcategory_sampling, how='left', on=['article_text', 'yearandmonth'])\n","all_products = pd.merge(all_products, df_category_sampling, how='left', on=['article_text', 'yearandmonth'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a17UgPjCpw2V"},"source":["all_products['rival_product'] = all_products.subcategory_rival.combine_first(all_products.category_rival)\n","all_products = all_products[['article_text', 'rival_product', 'yearandmonth']]\n","df_concat = pd.merge(df_concat, all_products, how='left', on=['article_text', 'yearandmonth'])\n","df_concat = df_concat[df_concat['rival_product'].notna()]\n","df_concat['product_index'] = df_concat.index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HquMvNBuWN4Q"},"source":["df_concat['product_id'] = df_concat['product_id'].astype('int')\n","df_concat = df_concat[['day'\t,'basket_hash',\t'user_id', 'quantity',\t'price',\t'user_visit_count', 'week', 'year', 'rival_product', 'product_index', 'article_text']]\n","df_concat = df_concat.melt(id_vars=['day'\t,'basket_hash',\t'user_id', 'quantity',\t'price',\t'user_visit_count', 'week', 'year', 'product_index'], \n","        var_name=\"indicator\", \n","        value_name=\"products\")\n","df_concat['bought'] = np.where(df_concat['indicator']== 'article_text', 1, 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qax3FAwDTUX8","executionInfo":{"status":"ok","timestamp":1614524313029,"user_tz":-60,"elapsed":489,"user":{"displayName":"Asena Ciloglu","photoUrl":"","userId":"06190869319946735371"}},"outputId":"64c9183c-8611-48c3-ee59-d08fe200f9ab"},"source":["product_ids = products[['article_text', 'product_id']]\n","product_ids.rename(columns={'article_text' : 'products'}, inplace=True)\n","product_ids = product_ids.drop_duplicates(subset=['products'])\n","df_concat = pd.merge(df_concat, product_ids, how='left', on=['products'])\n","df_concat.rename(columns={'products' : 'article_text'}, inplace=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"gQUFcX5MZNBR"},"source":["#### Add prices of negative sampled products"]},{"cell_type":"code","metadata":{"id":"91UgnooSat94"},"source":["#use price information from purchased products to fill in not purchased products:\n","df_concat_sold = df_concat[df_concat['bought'] == 1]\n","del df_concat_sold['week']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhh6OgELYP5P","executionInfo":{"status":"ok","timestamp":1614524318808,"user_tz":-60,"elapsed":652,"user":{"displayName":"Asena Ciloglu","photoUrl":"","userId":"06190869319946735371"}},"outputId":"7ffd41db-b77d-4ead-d52d-7d5e2633b1f9"},"source":["per_date_product_price = df_concat_sold[['day', 'product_id', 'price', 'year']]\n","per_date_product_price['day'] = pd.to_datetime(per_date_product_price['day'])\n","per_date_product_price['week'] = per_date_product_price['day'].dt.week"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ZDV0R5YwZREO"},"source":["price_per_week = per_date_product_price.groupby(['product_id', 'week', 'year'], as_index=False).agg(\n","    {'price': lambda x:stats.mode(x)[0]})\n","\n","week_and_year = per_date_product_price.groupby(['week', 'year'], as_index=False).agg(\n","    {})\n","week_and_year['key'] = 0\n","\n","product_df = pd.DataFrame(df_concat['product_id'].unique())\n","product_df['key'] = 0\n","product_df.rename(columns={0: 'product_id'}, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"en7AtWUuae-y"},"source":["merge_with_week_year = pd.DataFrame(week_and_year.merge(product_df, how='outer'))\n","merge_with_week_year.drop(columns=['key'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PUrC-eNKahF7"},"source":["merge_with_week_year['week'] = merge_with_week_year['week'].astype('str')\n","merge_with_week_year['year'] = merge_with_week_year['year'].astype('str')\n","merge_with_week_year['product_id'] = merge_with_week_year['product_id'].astype('str')\n","\n","price_per_week['product_id'] = price_per_week['product_id'].astype('str')\n","price_per_week['week'] = price_per_week['week'].astype('str')\n","price_per_week['year'] = price_per_week['year'].astype('str')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4IJ72LQialYV"},"source":["df_prices = pd.merge(merge_with_week_year, price_per_week, how='left', on=['product_id', 'week', 'year'])\n","df_prices['price2'] = df_prices.groupby(['product_id', 'year'])['price'].transform(lambda x: x.fillna(method = 'ffill'))\n","df_prices['price2'] = df_prices.groupby(['product_id', 'year'])['price2'].transform(lambda x: x.fillna(method = 'bfill'))\n","df_prices['price3'] = df_prices.groupby(['product_id'])['price2'].transform(lambda x: x.fillna(method = 'ffill'))\n","df_prices['price3'] = df_prices.groupby(['product_id'])['price3'].transform(lambda x: x.fillna(method = 'bfill'))\n","\n","df_prices.drop(['price', 'price2'], axis=1, inplace=True)\n","df_prices.rename(columns={'price3': 'price'}, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pJn8gFSTcxPc"},"source":["df_prices.rename(columns={'price' : 'fillingprice'}, inplace=True)\n","df_concat['product_id'] = df_concat['product_id'].astype('str')\n","# only change prices for not bought products\n","df_concat.loc[df_concat['bought'] == 0, 'price'] = 'empty'\n","df_concat = pd.merge(df_concat, df_prices, how='left', on=['product_id', 'week', 'year'])\n","\n","df_concat['fillingprice'] = df_concat['fillingprice'].astype('str')\n","df_concat.loc[df_concat['price'] == 'empty', 'price'] = df_concat['fillingprice']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"stFAqufPidW1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614524417930,"user_tz":-60,"elapsed":94323,"user":{"displayName":"Asena Ciloglu","photoUrl":"","userId":"06190869319946735371"}},"outputId":"3c6c0c4b-d7db-4e48-9432-3df6ef44745b"},"source":["products_add = products[['product_id', 'subcategory_name', 'category_name']]\n","products_add['product_id'] = products_add['product_id'].astype('str')\n","df_concat = pd.merge(df_concat, products_add, how='left', on=['product_id'])\n","df_concat.loc[df_concat['bought'] == 0, 'quantity'] = 0\n","del df_concat['user_visit_count'], df_concat['indicator'], df_concat['fillingprice']\n","df_concat.rename(columns={'products' : 'article_text'}, inplace=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"_yrYl22OD5c2"},"source":["## Feature Engineering"]},{"cell_type":"code","metadata":{"id":"lCgnZwNda55W"},"source":["train = df_concat[:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qD6mkedKwBvw"},"source":["train['day'] = pd.to_datetime(train['day'])\n","train['week'] = train['week'].str.zfill(2)\n","train[\"weekandyear\"] = train[\"year\"] + train[\"week\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-MWW4Mb2jclA"},"source":["#### Label Encoding"]},{"cell_type":"code","metadata":{"id":"Jw5znK1_L-l7"},"source":["train['product_id_notenc'] = train['product_id']\n","train['day_notenc'] = train['day']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"reY6_5P_qrtC"},"source":["# creating label encoders for categorical features\n","\n","le = LabelEncoder()\n","le.fit(train['product_id'])\n","train['product_id'] = le.transform(train['product_id'])\n","\n","le = LabelEncoder()\n","le.fit(train['user_id'])\n","train['user_id'] = le.transform(train['user_id'])\n","\n","le_day = LabelEncoder()\n","le_day.fit(train['day'])\n","train['day'] = le_day.transform(train['day'])\n","\n","le_day = LabelEncoder()\n","le_day.fit(train['basket_hash'])\n","train['basket_hash'] = le_day.transform(train['basket_hash'])\n","\n","le_sub = LabelEncoder()\n","le_sub.fit(train['subcategory_name'])\n","train['subcategory_name'] = le_sub.transform(train['subcategory_name'])\n","\n","le_sub = LabelEncoder()\n","le_sub.fit(train['category_name'])\n","train['category_name'] = le_sub.transform(train['category_name'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zNyYq9VqJBMn"},"source":["#### Shopping baskets"]},{"cell_type":"code","metadata":{"id":"_3KbmXNwJGON"},"source":["def trp(l, n):\n","    return [0]*(n-len(l)) + l[:n]\n","\n","#get baskets\n","only_bought = train[train['bought'] == 1]\n","baskets = only_bought.groupby(['basket_hash'], as_index=False).aggregate({'product_id' : 'unique'})\n","baskets.rename(columns={'product_id' : 'basket'}, inplace=True)\n","train = pd.merge(train, baskets, how='left', on=['basket_hash'])\n","\n","basket = pad_sequences(train['basket'],35)\n","train['basket'] = basket.tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MVfadOhzwIi9"},"source":["#### Purchase history"]},{"cell_type":"code","metadata":{"id":"n_pJfY-iXdeZ"},"source":["train['day'] = train['day'].astype('str')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QXjHc6AfwK1l"},"source":["bought_df = train[train['bought'] == 1]\n","purchase_history = bought_df.groupby(['product_id', 'user_id'], as_index=False).aggregate({'day' : 'unique'})\n","purchase_history.rename(columns={'day' : 'purchase_history'}, inplace=True)\n","# merge with actual df\n","train = pd.merge(train, purchase_history, how='left', on=['user_id', 'product_id'])\n","\n","user_history = bought_df.groupby(['user_id'], as_index=False).aggregate({'day' : 'unique'})\n","user_history.rename(columns={'day' : 'user_history'}, inplace=True)\n","user_history['user_history'] = user_history['user_history'].map(lambda x: np.sort(x))\n","train = pd.merge(train, user_history, how='left', on=['user_id'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cffBe5PrgoME"},"source":["train['user_history'] = train['user_history'].map(lambda x: [int(i) for i in x])\n","train['user_history'] = train['user_history'].map(lambda x: np.sort(x))\n","train['day'] = train['day'].astype('int')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NmmbjxU5wUGy"},"source":["def get_last_30(seq, value):\n","     lst = []\n","     for s in seq:\n","         if s < value:\n","             lst.append(s)\n","     return lst[-30:]\n","\n","train['last_30_purchase_days'] = train.apply(lambda x: get_last_30(x.user_history, x.day), axis=1)\n","train['purchase_history'] = np.where(train['purchase_history'].isna(), \"\", train['purchase_history'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iyermnAVvuCL"},"source":["train['purchase_history'] = train['purchase_history'].map(lambda x: [int(i) for i in x])\n","train['purchase_history'] = train['purchase_history'].map(lambda x: np.sort(x))\n","\n","train['purchase_history'] = train['purchase_history'].map(lambda x: [str(i) for i in x])\n","train['last_30_purchase_days'] = train['last_30_purchase_days'].map(lambda x: [str(i) for i in x])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tUBtRXUlwWKI"},"source":["train['30_day_purchase_history'] = train.apply(lambda x: [1 if str(i) in x.purchase_history else -1 for i in x.last_30_purchase_days], axis=1)\n","\n","padded_window_30 = pad_sequences(train['30_day_purchase_history'],30,padding='pre')\n","train['30_day_purchase_history'] = padded_window_30.tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"djDt_rfSPqVQ"},"source":["#### Train test split"]},{"cell_type":"code","metadata":{"id":"HpOYoQikFDgQ"},"source":["last_purchase_day_for_user = train.groupby('user_id', as_index=False).aggregate({'day' : 'max'})\n","last_purchase_day_for_user.rename(columns=({'day' : 'last_purchase_day'}), inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wnqu59kDFIgE"},"source":["train = pd.merge(train, last_purchase_day_for_user, how='left', on=['user_id'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Kr9WGoDFiOV"},"source":["train['data_split'] = np.where(train['day']==train['last_purchase_day'], 'test', 'train')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3lAYOchTKSZR"},"source":["df_train = train[train['data_split'] == 'train']\n","df_test = train[train['data_split'] == 'test']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LJADtWOPw9RW"},"source":["#### Price attributes"]},{"cell_type":"code","metadata":{"id":"1S7xP9rdIhcx"},"source":["df_train['price'] = df_train['price'].astype('float')\n","df_train['quantity'] = df_train['quantity'].astype('float')\n","\n","df_test['price'] = df_test['price'].astype('float')\n","df_test['quantity'] = df_test['quantity'].astype('float')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rb8RTuXTxFGd"},"source":["# get black prices\n","train_df_bought = df_train[df_train['bought'] == 1]\n","black_prices = train_df_bought.groupby('product_id', as_index=False).aggregate({'price' : 'max'})\n","black_prices.rename(columns=({'price' : 'black_price'}), inplace=True)\n","\n","df_train = pd.merge(df_train, black_prices, how='left', on=['product_id'])\n","\n","df_train['price'] = df_train['price'].astype('float')\n","df_train['discount'] = (df_train['black_price'] -  df_train['price'])/df_train['black_price']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VYluDH2IxIT9"},"source":["# get black price to test data and calculate discount\n","df_test = pd.merge(df_test, black_prices, how='left', on=['product_id'])\n","df_test['price'] = df_test['price'].astype('float')\n","df_test['discount'] = (df_test['black_price'] -  df_test['price'])/df_test['black_price']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nwuGKLg9e7o4"},"source":["df_train['discount'].fillna(0.000000, inplace=True)\n","df_test['discount'].fillna(0.000000, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0xa7e7Eijp7q"},"source":["#### Other products in the basket"]},{"cell_type":"code","metadata":{"id":"4ZTKoQ4beVSx"},"source":["#get other products in the basket\n","only_bought = df_train[df_train['bought'] == 1]\n","baskets = only_bought.groupby(['basket_hash'], as_index=False).aggregate({'product_id' : 'unique'})\n","baskets.rename(columns={'product_id' : 'basket'}, inplace=True)\n","only_bought = pd.merge(only_bought, baskets, how='left', on=['basket_hash'])\n","only_bought['other_prods_in_basket'] = only_bought.apply(lambda x: [i for i in x.basket if i != x.product_id], axis=1)\n","only_bought['product_index'] = only_bought['product_index'].astype('str')\n","only_bought['basket_hash'] = only_bought['basket_hash'].astype('str')\n","only_bought['basket_index'] = only_bought[['basket_hash', 'product_index']].agg('_'.join, axis=1)\n","only_bought_basket_info = only_bought[['other_prods_in_basket', 'basket_index']]\n","df_train['product_index'] = df_train['product_index'].astype('str')\n","df_train['basket_hash'] = df_train['basket_hash'].astype('str')\n","df_train['basket_index'] = df_train[['basket_hash', 'product_index']].agg('_'.join, axis=1)\n","df_train = pd.merge(df_train, only_bought_basket_info, how='left', on=['basket_index'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QYFv21mh5RPK"},"source":["#get other products in the basket for test data\n","only_bought = df_test[df_test['bought'] == 1]\n","baskets = only_bought.groupby(['basket_hash'], as_index=False).aggregate({'product_id' : 'unique'})\n","baskets.rename(columns={'product_id' : 'basket'}, inplace=True)\n","only_bought = pd.merge(only_bought, baskets, how='left', on=['basket_hash'])\n","only_bought['other_prods_in_basket'] = only_bought.apply(lambda x: [i for i in x.basket if i != x.product_id], axis=1)\n","only_bought['product_index'] = only_bought['product_index'].astype('str')\n","only_bought['basket_hash'] = only_bought['basket_hash'].astype('str')\n","only_bought['basket_index'] = only_bought[['basket_hash', 'product_index']].agg('_'.join, axis=1)\n","only_bought_basket_info = only_bought[['other_prods_in_basket', 'basket_index']]\n","df_test['product_index'] = df_test['product_index'].astype('str')\n","df_test['basket_hash'] = df_test['basket_hash'].astype('str')\n","df_test['basket_index'] = df_test[['basket_hash', 'product_index']].agg('_'.join, axis=1)\n","df_test = pd.merge(df_test, only_bought_basket_info, how='left', on=['basket_index'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aQ6ZLQ8lJZw3"},"source":["df_test['other_prods_in_basket'] = df_test.other_prods_in_basket.apply(lambda x: np.array(x, dtype=np.int))\n","df_train['other_prods_in_basket'] = df_train.other_prods_in_basket.apply(lambda x: np.array(x, dtype=np.int))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sxEXW1SfdgG2"},"source":["def trp(l, n):\n","    return [0]*(n-len(l)) + l[:n]\n","\n","df_train['other_prods_in_basket'] = df_train.other_prods_in_basket.apply(lambda x: trp(x, 35))\n","df_test['other_prods_in_basket'] = df_test.other_prods_in_basket.apply(lambda x: trp(x, 35))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TTYvY4twpYQ3"},"source":["#### Sample weights"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ceI4Snv2paln","executionInfo":{"status":"ok","timestamp":1619383849070,"user_tz":-120,"elapsed":203299,"user":{"displayName":"Asena Çiloğlu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl3K-NLZoPkBt0kyRj3L4b-TAHeujunPYtFDZ8kg=s64","userId":"17471100050893812089"}},"outputId":"57d6a789-c6ef-481f-b7c3-0c0602793278"},"source":["df_bought = df_train[df_train['bought'] == 1]\n","df_bought['total_purchases'] = df_bought.groupby('product_id')['product_id'].transform('count')\n","product_frequency = df_bought[['product_id', 'total_purchases']].drop_duplicates()\n","df_train = pd.merge(df_train, product_frequency, on=['product_id'], how='left')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"sTN1PHr5pfRu"},"source":["min_max = MinMaxScaler()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AQ0Xhj3uphhm"},"source":["for_not_bought = min_max.fit_transform(pd.DataFrame(df_train['total_purchases']))\n","df_train['for_not_bought'] = for_not_bought"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SaD0-tlmpjQ2"},"source":["df_train['for_bought'] = 1 / df_train['total_purchases']\n","for_bought = min_max.fit_transform(pd.DataFrame(df_train['for_bought']))\n","df_train['for_bought'] = for_bought"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oS0KMezxp04j"},"source":["df_train['sample_weight'] = np.where(df_train['bought'] == 1, df_train['for_bought'], df_train['for_not_bought'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ODJq4g25p2M5"},"source":["We will use the calculated sample weights for train data for test data as well."]},{"cell_type":"code","metadata":{"id":"67bNZl-8p9AH"},"source":["train_weights = df_train[['product_id', 'total_purchases', 'for_bought', 'for_not_bought']]\n","train_weights = train_weights.drop_duplicates()\n","df_test = pd.merge(df_test, train_weights, how='left', on=['product_id'])\n","df_test['sample_weight'] = np.where(df_test['bought'] == 1, df_test['for_bought'], df_test['for_not_bought'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BYEjQDP3qxH5"},"source":["#### Difference in discounts per substitute products"]},{"cell_type":"code","metadata":{"id":"DjWWqTA3qy_z"},"source":["df_train['diffs'] = df_train.groupby(['product_index'])['discount'].transform(lambda x: x.diff())\n","df_train[\"diffs\"] = df_train.groupby(\"product_index\")['diffs'].transform(lambda x: x.fillna(x.mean()*-1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dKhFTwdkq5_B"},"source":["df_test['diffs'] = df_test.groupby(['product_index'])['discount'].transform(lambda x: x.diff())\n","df_test[\"diffs\"] = df_test.groupby(\"product_index\")['diffs'].transform(lambda x: x.fillna(x.mean()*-1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mTqY2cI_dxbv"},"source":["#### Save data as a .pkl file"]},{"cell_type":"code","metadata":{"id":"LBR8zqeuey_A"},"source":["#merge test and train data\n","df_train['data_split'] = 'train'\n","df_test['data_split'] = 'test'\n","\n","df = df_train.append(df_test, ignore_index=True)\n","\n","del df['purchase_history'], df['user_history'], df['last_30_purchase_days'], df['last_purchase_day'], df['black_price'],df['basket_index'],df['for_not_bought'],df['for_bought']     "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mU12hqepvgxe"},"source":["with open('data_preprocessed.pkl', 'wb') as handle:\n","    pickle.dump(df, handle)\n","\n","from google.colab import files\n","files.download('data_preprocessed.pkl')"],"execution_count":null,"outputs":[]}]}